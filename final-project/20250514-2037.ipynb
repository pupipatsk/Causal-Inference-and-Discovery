{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 1186\n",
      "After dedup: 213764\n",
      "Age 0 count: 3912\n",
      "After age filter: 209852\n",
      "Survival distribution:\n",
      " survived\n",
      "1    206365\n",
      "0      3487\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Q1: Feature Importances ===\n",
      "Top 10 factors affecting survivability:\n",
      " hour                      0.342769\n",
      "day_of_month              0.140318\n",
      "อายุ                      0.078220\n",
      "การดื่มสุรา_ไม่ทราบ       0.054849\n",
      "sex                       0.028433\n",
      "ถนนที่เกิดเหตุ_ทางหลวง    0.025287\n",
      "month                     0.022746\n",
      "รถผู้บาดเจ็บ_ปิคอัพ       0.019391\n",
      "ถนนที่เกิดเหตุ_ในเมือง    0.018606\n",
      "สถานะ_ผู้ชับขี่           0.017952\n",
      "dtype: float64\n",
      "\n",
      "=== Q2: Helmet Effect ===\n",
      "helmet ATE: 0.0064\n",
      "Refute: Use a Placebo Treatment\n",
      "Estimated effect:0.006385687527912387\n",
      "New effect:0.00010765016030875255\n",
      "p value:0.96\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 202\u001b[39m\n\u001b[32m    199\u001b[39m     plot_hourly_survival(df)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 177\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    175\u001b[39m df_mc_enc = pd.get_dummies(df_mc, columns=[\u001b[33m'\u001b[39m\u001b[33mถนนที่เกิดเหตุ\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mการดื่มสุรา\u001b[39m\u001b[33m'\u001b[39m], drop_first=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    176\u001b[39m conf = [\u001b[33m'\u001b[39m\u001b[33mอายุ\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33msex\u001b[39m\u001b[33m'\u001b[39m] + [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_mc_enc.columns \u001b[38;5;28;01mif\u001b[39;00m c.startswith((\u001b[33m'\u001b[39m\u001b[33mถนนที่เกิดเหตุ_\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mการดื่มสุรา_\u001b[39m\u001b[33m'\u001b[39m))]\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[43mestimate_causal_effect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_mc_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhelmet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msurvived\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Q3: Seatbelt Effect ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    180\u001b[39m car_types = [\u001b[33m'\u001b[39m\u001b[33mรถเก๋ง/แท็กซี่\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mปิคอัพ\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mรถตู้\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mestimate_causal_effect\u001b[39m\u001b[34m(df, treatment, outcome, confounders)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Refutations\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28mprint\u001b[39m(model.refute_estimate(estimand, est, method_name=\u001b[33m\"\u001b[39m\u001b[33mplacebo_treatment_refuter\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrefute_estimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrandom_common_cause\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    126\u001b[39m naive = data[data[treatment] == \u001b[32m1\u001b[39m][outcome].mean() - data[data[treatment] == \u001b[32m0\u001b[39m][outcome].mean()\n\u001b[32m    127\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNaive diff-in-means: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnaive\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/dowhy/causal_model.py:460\u001b[39m, in \u001b[36mCausalModel.refute_estimate\u001b[39m\u001b[34m(self, estimand, estimate, method_name, show_progress_bar, **kwargs)\u001b[39m\n\u001b[32m    457\u001b[39m     refuter_class = causal_refuters.get_class_object(method_name)\n\u001b[32m    459\u001b[39m refuter = refuter_class(\u001b[38;5;28mself\u001b[39m._data, identified_estimand=estimand, estimate=estimate, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m res = \u001b[43mrefuter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrefute_estimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/dowhy/causal_refuters/random_common_cause.py:45\u001b[39m, in \u001b[36mRandomCommonCause.refute_estimate\u001b[39m\u001b[34m(self, show_progress_bar)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrefute_estimate\u001b[39m(\u001b[38;5;28mself\u001b[39m, show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     refute = \u001b[43mrefute_random_common_cause\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_target_estimand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_estimate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_simulations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_random_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_n_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     refute.add_refuter(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m refute\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/dowhy/causal_refuters/random_common_cause.py:119\u001b[39m, in \u001b[36mrefute_random_common_cause\u001b[39m\u001b[34m(data, target_estimand, estimate, num_simulations, random_state, show_progress_bar, n_jobs, verbose, **_)\u001b[39m\n\u001b[32m    116\u001b[39m     random_state = np.random.RandomState(seed=random_state)\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Run refutation in parallel\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m sample_estimates = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_refute_once\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midentified_estimand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolour\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCausalRefuter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPROGRESS_BAR_COLOR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRefuting Estimates: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m sample_estimates = np.array(sample_estimates)\n\u001b[32m    130\u001b[39m refute = CausalRefutation(\n\u001b[32m    131\u001b[39m     estimate.value, np.mean(sample_estimates), refutation_type=\u001b[33m\"\u001b[39m\u001b[33mRefute: Add a random common cause\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/dowhy/causal_refuters/random_common_cause.py:76\u001b[39m, in \u001b[36m_refute_once\u001b[39m\u001b[34m(data, target_estimand, estimate, random_state)\u001b[39m\n\u001b[32m     70\u001b[39m new_estimator = estimate.estimator.get_new_estimator_object(target_estimand)\n\u001b[32m     71\u001b[39m new_estimator.fit(\n\u001b[32m     72\u001b[39m     new_data,\n\u001b[32m     73\u001b[39m     effect_modifier_names=estimate.estimator._effect_modifier_names,\n\u001b[32m     74\u001b[39m     **new_estimator._fit_params \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(new_estimator, \u001b[33m\"\u001b[39m\u001b[33m_fit_params\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {},\n\u001b[32m     75\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m new_effect = \u001b[43mnew_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimate_effect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrol_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrol_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtreatment_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtreatment_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_units\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_target_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_effect.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/dowhy/causal_estimators/propensity_score_matching_estimator.py:128\u001b[39m, in \u001b[36mPropensityScoreMatchingEstimator.estimate_effect\u001b[39m\u001b[34m(self, data, treatment_value, control_value, target_units, **_)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# TODO remove neighbors that are more than a given radius apart\u001b[39;00m\n\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Estimating ATT on treated by summing over difference between matched neighbors\u001b[39;00m\n\u001b[32m    125\u001b[39m control_neighbors = NearestNeighbors(n_neighbors=\u001b[32m1\u001b[39m, algorithm=\u001b[33m\"\u001b[39m\u001b[33mball_tree\u001b[39m\u001b[33m\"\u001b[39m).fit(\n\u001b[32m    126\u001b[39m     control[\u001b[38;5;28mself\u001b[39m.propensity_score_column].values.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    127\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m distances, indices = \u001b[43mcontrol_neighbors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtreated\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpropensity_score_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mdistances:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    130\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.debug(distances)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/sklearn/neighbors/_base.py:903\u001b[39m, in \u001b[36mKNeighborsMixin.kneighbors\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    898\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[32m    899\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    900\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m does not work with sparse matrices. Densify the data, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    901\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mor set algorithm=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mself\u001b[39m._fit_method\n\u001b[32m    902\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m     chunked_results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_tree_query_parallel_helper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    910\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33minternal: _fit_method not recognized\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/sklearn/utils/parallel.py:74\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     69\u001b[39m config = get_config()\n\u001b[32m     70\u001b[39m iterable_with_config = (\n\u001b[32m     71\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     73\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/sklearn/utils/parallel.py:136\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m     config = {}\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/sklearn/neighbors/_base.py:704\u001b[39m, in \u001b[36m_tree_query_parallel_helper\u001b[39m\u001b[34m(tree, *args, **kwargs)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_tree_query_parallel_helper\u001b[39m(tree, *args, **kwargs):\n\u001b[32m    699\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Helper for the Parallel calls in KNeighborsMixin.kneighbors.\u001b[39;00m\n\u001b[32m    700\u001b[39m \n\u001b[32m    701\u001b[39m \u001b[33;03m    The Cython method tree.query is not directly picklable by cloudpickle\u001b[39;00m\n\u001b[32m    702\u001b[39m \u001b[33;03m    under PyPy.\u001b[39;00m\n\u001b[32m    703\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np  # numpy documentation: https://numpy.org/doc/\n",
    "import pandas as pd  # pandas documentation: https://pandas.pydata.org/docs/\n",
    "import matplotlib.pyplot as plt  # matplotlib documentation: https://matplotlib.org/stable/api/pyplot_api.html\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  # scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.linear_model import LogisticRegression  # scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dowhy import CausalModel  # dowhy: https://microsoft.github.io/dowhy/\n",
    "\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load raw Excel data.\"\"\"\n",
    "    return pd.read_excel(path)\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform data cleaning:\n",
    "    1. Remove duplicates.\n",
    "    2. Filter valid ages.\n",
    "    3. Create binary survival label.\n",
    "    4. Extract month and hour correctly.\n",
    "    5. Encode sex.\n",
    "    6. Drop unneeded columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # 1. Duplicates\n",
    "    print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"After dedup: {df.shape[0]}\")\n",
    "    \n",
    "    # 2. Age validity\n",
    "    # Convert to numeric and filter invalid ages\n",
    "    df['อายุ'] = pd.to_numeric(df['อายุ'], errors='coerce')\n",
    "    print(f\"Age 0 count: {(df['อายุ'] == 0).sum()}\")\n",
    "    df = df[df['อายุ'] > 0].dropna(subset=['อายุ'])\n",
    "    print(f\"After age filter: {df.shape[0]}\")\n",
    "\n",
    "    # Define bins and their midpoints\n",
    "    age_bins = [0, 15, 25, 65, 200]\n",
    "    bin_midpoints = {\n",
    "        pd.Interval(0, 15, closed='left'): 7.5,\n",
    "        pd.Interval(15, 25, closed='left'): 20,\n",
    "        pd.Interval(25, 65, closed='left'): 45,\n",
    "        pd.Interval(65, 200, closed='left'): 132.5\n",
    "    }\n",
    "\n",
    "    # Bin ages and map to midpoints\n",
    "    binned = pd.cut(df['อายุ'], bins=age_bins, right=False)\n",
    "    df['อายุ'] = binned.map(bin_midpoints)\n",
    "    \n",
    "    # 3. Survival label\n",
    "    df['survived'] = (df['ผลการรักษา'] == 'ทุเลา/หาย').astype(int)\n",
    "    print(\"Survival distribution:\\n\", df['survived'].value_counts())\n",
    "    df = df.drop(columns=['ผลการรักษา'])\n",
    "    \n",
    "    # 4. Date/Time processing\n",
    "    df = df.rename(columns={'วันที่เกิดเหตุ': 'day_of_month'})\n",
    "    df = df[df['day_of_month'].between(1, 31)]\n",
    "    # Map month for New Year period: days 29-31 -> Dec(12), days 1-4 -> Jan(1)\n",
    "    df['month'] = df['day_of_month'].apply(lambda x: 12 if x >= 29 else (1 if x <= 4 else np.nan))\n",
    "    df = df.dropna(subset=['month'])\n",
    "    df['month'] = df['month'].astype(int)\n",
    "    \n",
    "    def extract_hour(s):\n",
    "        if pd.isna(s) or 'ไม่ทราบ' in str(s):\n",
    "            return np.nan\n",
    "        cleaned = re.sub(r'[^0-9:]', '', str(s))\n",
    "        if cleaned.startswith('24:'):\n",
    "            cleaned = '00:' + cleaned[3:]\n",
    "        try:\n",
    "            return int(cleaned.split(':')[0])\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    \n",
    "    df['hour'] = df['เวลาเกิดเหตุ'].apply(extract_hour)\n",
    "    df = df.drop(columns=['เวลาเกิดเหตุ'])\n",
    "    \n",
    "    # 5. Gender encoding\n",
    "    df['sex'] = df['เพศ'].map({'ชาย': 1, 'หญิง': 0})\n",
    "    df = df.drop(columns=['เพศ'])\n",
    "    \n",
    "    # 6. Drop irrelevant columns\n",
    "    df = df.drop(columns=['จังหวัด', 'ชื่อโรงพยาบาลที่รับผู้บาดเจ็บ', 'ชื่อเทศกาล'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_importance_rf(df: pd.DataFrame):\n",
    "    \"\"\"Compute and display top-5 feature importances via Random Forest.\"\"\"\n",
    "    feats = ['อายุ', 'sex', 'day_of_month', 'month', 'hour',\n",
    "             'ถนนที่เกิดเหตุ', 'สถานะ', 'รถผู้บาดเจ็บ',\n",
    "             'รถคู่กรณี', 'มาตรการ', 'การดื่มสุรา']\n",
    "    sub = df[feats + ['survived']].dropna(subset=['hour'])\n",
    "    sub = pd.get_dummies(sub, columns=[\n",
    "        'ถนนที่เกิดเหตุ', 'สถานะ', 'รถผู้บาดเจ็บ',\n",
    "        'รถคู่กรณี', 'มาตรการ', 'การดื่มสุรา'\n",
    "    ], drop_first=True)\n",
    "    X = sub.drop(columns=['survived'])\n",
    "    y = sub['survived']\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    imp = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    print(\"Top 10 factors affecting survivability:\\n\", imp.sort_values(ascending=False).head(10))\n",
    "\n",
    "\n",
    "def estimate_causal_effect(df: pd.DataFrame, treatment: str, outcome: str, confounders: list):\n",
    "    \"\"\"\n",
    "    Estimate ATE with DoWhy (PSM) and run two refutation tests.\n",
    "    \"\"\"\n",
    "    data = df.dropna(subset=[treatment, outcome] + confounders)\n",
    "    model = CausalModel(\n",
    "        data=data,\n",
    "        treatment=treatment,\n",
    "        outcome=outcome,\n",
    "        common_causes=confounders\n",
    "    )\n",
    "    estimand = model.identify_effect()\n",
    "    est = model.estimate_effect(estimand, method_name=\"backdoor.propensity_score_matching\")\n",
    "    print(f\"{treatment} ATE: {est.value:.4f}\")\n",
    "    # Refutations\n",
    "    print(model.refute_estimate(estimand, est, method_name=\"placebo_treatment_refuter\"))\n",
    "    print(model.refute_estimate(estimand, est, method_name=\"random_common_cause\"))\n",
    "    naive = data[data[treatment] == 1][outcome].mean() - data[data[treatment] == 0][outcome].mean()\n",
    "    print(f\"Naive diff-in-means: {naive:.4f}\")\n",
    "\n",
    "\n",
    "def hospital_effect(df: pd.DataFrame):\n",
    "    \"\"\"Assess hospital-level deviations from expected survival.\"\"\"\n",
    "    feats = ['อายุ', 'sex', 'ถนนที่เกิดเหตุ', 'สถานะ',\n",
    "             'รถผู้บาดเจ็บ', 'รถคู่กรณี', 'มาตรการ', 'การดื่มสุรา']\n",
    "    sub = df[feats + ['survived', 'รหัส รพ.']].dropna()\n",
    "    sub_enc = pd.get_dummies(sub, columns=feats, drop_first=True)\n",
    "    X = sub_enc.drop(columns=['survived', 'รหัส รพ.'])\n",
    "    y = sub_enc['survived']\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    log = LogisticRegression(max_iter=5000, solver='liblinear').fit(X_scaled, y)\n",
    "    sub['expected'] = log.predict_proba(X_scaled)[:, 1]\n",
    "    rates = sub.groupby('รหัส รพ.').agg(\n",
    "        observed=('survived', 'mean'),\n",
    "        expected=('expected', 'mean')\n",
    "    )\n",
    "    rates['difference'] = rates['observed'] - rates['expected']\n",
    "    print(\"Hospital survival difference std:\", rates['difference'].std())\n",
    "\n",
    "\n",
    "def plot_hourly_survival(df: pd.DataFrame):\n",
    "    \"\"\"Plot survival rate by hour of day.\"\"\"\n",
    "    hr = (\n",
    "        df[df['hour'].notnull()]\n",
    "        .assign(hour=lambda x: x['hour'].astype(int))\n",
    "        .groupby('hour')['survived']\n",
    "        .mean()\n",
    "    )\n",
    "    hr.plot(title='Survival Rate by Hour', xlabel='Hour', ylabel='Survival Rate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    repo = os.path.dirname(os.getcwd())\n",
    "    path = os.path.join(repo, 'final-project', 'data', 'raw.xlsx')\n",
    "    df = load_data(path)\n",
    "    df = clean_data(df)\n",
    "\n",
    "    print(\"\\n=== Q1: Feature Importances ===\")\n",
    "    feature_importance_rf(df)\n",
    "\n",
    "    print(\"\\n=== Q2: Helmet Effect ===\")\n",
    "    df_mc = df[df['รถผู้บาดเจ็บ'] == 'จักรยานยนต์'].copy()\n",
    "    df_mc = df_mc[df_mc['มาตรการ'].notna()]\n",
    "    df_mc['helmet'] = (df_mc['มาตรการ'] == 'ใส่หมวก').astype(int)\n",
    "    df_mc_enc = pd.get_dummies(df_mc, columns=['ถนนที่เกิดเหตุ','การดื่มสุรา'], drop_first=True)\n",
    "    conf = ['อายุ','sex'] + [c for c in df_mc_enc.columns if c.startswith(('ถนนที่เกิดเหตุ_','การดื่มสุรา_'))]\n",
    "    estimate_causal_effect(df_mc_enc, 'helmet', 'survived', conf)\n",
    "\n",
    "    print(\"\\n=== Q3: Seatbelt Effect ===\")\n",
    "    car_types = ['รถเก๋ง/แท็กซี่', 'ปิคอัพ', 'รถตู้']\n",
    "    df_car = df[df['รถผู้บาดเจ็บ'].isin(car_types)].copy()\n",
    "    df_car = df_car[df_car['มาตรการ'].notna()]\n",
    "    df_car['seatbelt'] = (df_car['มาตรการ'] == 'เข็มขัด').astype(int)\n",
    "    df_car_enc = pd.get_dummies(df_car, columns=['ถนนที่เกิดเหตุ','การดื่มสุรา'], drop_first=True)\n",
    "    conf_car = ['อายุ','sex'] + [c for c in df_car_enc.columns if c.startswith(('ถนนที่เกิดเหตุ_','การดื่มสุรา_'))]\n",
    "    estimate_causal_effect(df_car_enc, 'seatbelt', 'survived', conf_car)\n",
    "\n",
    "    print(\"\\n=== Q4: Alcohol Effect ===\")\n",
    "    df_al = df[df['การดื่มสุรา'].notna()].copy()\n",
    "    df_al['alcohol'] = (df_al['การดื่มสุรา'] == 'ดื่ม').astype(int)\n",
    "    df_al_enc = pd.get_dummies(df_al, columns=['ถนนที่เกิดเหตุ','รถผู้บาดเจ็บ'], drop_first=True)\n",
    "    conf_al = ['อายุ','sex'] + [c for c in df_al_enc.columns if c.startswith(('ถนนที่เกิดเหตุ_','รถผู้บาดเจ็บ_'))]\n",
    "    estimate_causal_effect(df_al_enc, 'alcohol', 'survived', conf_al)\n",
    "\n",
    "    print(\"\\n=== Q5: Hospital Effect ===\")\n",
    "    hospital_effect(df)\n",
    "\n",
    "    print(\"\\n=== Q6: Hourly Survival ===\")\n",
    "    plot_hourly_survival(df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

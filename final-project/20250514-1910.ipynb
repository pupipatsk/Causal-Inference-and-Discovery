{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === iPython Config ===\n",
    "from IPython import get_ipython\n",
    "if 'IPython.extensions.autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "else:\n",
    "    get_ipython().run_line_magic('reload_ext', 'autoreload')\n",
    "%autoreload 2\n",
    "\n",
    "# === System and Path ===\n",
    "import os\n",
    "import sys\n",
    "repo_path = os.path.dirname(os.getcwd())\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === Import libraries ===\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load data ===\n",
    "file_path = os.path.join(repo_path, \"final-project\", \"data\", \"raw.xlsx\")\n",
    "df_raw = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 214950 entries, 0 to 214949\n",
      "Data columns (total 19 columns):\n",
      " #   Column                         Non-Null Count   Dtype \n",
      "---  ------                         --------------   ----- \n",
      " 0   ชื่อเทศกาล                     214950 non-null  object\n",
      " 1   รหัสจังหวัด                    214950 non-null  int64 \n",
      " 2   จังหวัด                        214950 non-null  object\n",
      " 3   รหัส รพ.                       214950 non-null  int64 \n",
      " 4   ชื่อโรงพยาบาลที่รับผู้บาดเจ็บ  214950 non-null  object\n",
      " 5   วันที่เกิดเหตุ                 214950 non-null  int64 \n",
      " 6   เวลาเกิดเหตุ                   214950 non-null  object\n",
      " 7   เพศ                            214950 non-null  object\n",
      " 8   อายุ                           214950 non-null  int64 \n",
      " 9   ถนนที่เกิดเหตุ                 214950 non-null  object\n",
      " 10  สถานะ                          214950 non-null  object\n",
      " 11  รถผู้บาดเจ็บ                   214950 non-null  object\n",
      " 12  รถคู่กรณี                      214950 non-null  object\n",
      " 13  มาตรการ                        214950 non-null  object\n",
      " 14  การดื่มสุรา                    214950 non-null  object\n",
      " 15  การนำส่ง                       214950 non-null  object\n",
      " 16  Refer-Admit                    214950 non-null  object\n",
      " 17  ผลการรักษา                     214950 non-null  object\n",
      " 18  จำนวนวันรักษา                  214950 non-null  int64 \n",
      "dtypes: int64(5), object(14)\n",
      "memory usage: 31.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleaning Duplicate Rows ===\n",
      "Duplicate records: 1186\n",
      "Remaining records after removing duplicates: 213764\n"
     ]
    }
   ],
   "source": [
    "def clean_duplicate_rows(df):\n",
    "    print(\"=== Cleaning Duplicate Rows ===\")\n",
    "    dup_count = df.duplicated().sum()\n",
    "    print(f\"Duplicate records: {dup_count}\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Remaining records after removing duplicates: {df.shape[0]}\")\n",
    "    return df\n",
    "\n",
    "df_cleaned = clean_duplicate_rows(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleaning Age Validity ===\n",
      "Records with age = 0: 3912\n",
      "Remaining records after removing age = 0: 209852\n"
     ]
    }
   ],
   "source": [
    "def clean_age_validity(df):\n",
    "    print(\"=== Cleaning Age Validity ===\")\n",
    "    print(f\"Records with age = 0: {df[df['อายุ'] == 0].shape[0]}\")\n",
    "    df = df[df['อายุ'] > 0]\n",
    "    print(f\"Remaining records after removing age = 0: {df.shape[0]}\")\n",
    "    return df\n",
    "\n",
    "df_cleaned = clean_age_validity(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating 'survived' column ===\n",
      "survived\n",
      "1    206365\n",
      "0      3487\n",
      "Name: count, dtype: int64\n",
      "=== Dropping 'ผลการรักษา' column ===\n"
     ]
    }
   ],
   "source": [
    "def create_survival_label(df):\n",
    "    \"\"\"\n",
    "    Create a binary 'survived' column:\n",
    "    - 1 if 'ผลการรักษา' == 'ทุเลา/หาย'\n",
    "    - 0 otherwise (any form of death)\n",
    "    \"\"\"\n",
    "    print(\"=== Creating 'survived' column ===\")\n",
    "    df['survived'] = (df['ผลการรักษา'] == 'ทุเลา/หาย').astype(int)\n",
    "    print(df['survived'].value_counts())\n",
    "    # drop the 'ผลการรักษา' column\n",
    "    print(\"=== Dropping 'ผลการรักษา' column ===\")\n",
    "    df = df.drop(columns=['ผลการรักษา'])\n",
    "    return df\n",
    "\n",
    "df_cleaned = create_survival_label(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleaning and Combining to 'datetime' ===\n",
      "Dropped 'ชื่อเทศกาล' column.\n",
      "Extracted 'hour' column.\n",
      "Dropped 'เวลาเกิดเหตุ' columns.\n",
      "Dropped 'year' column.\n"
     ]
    }
   ],
   "source": [
    "def clean_datetime(df):\n",
    "    \"\"\"\n",
    "    Combines 'ชื่อเทศกาล', 'วันที่เกิดเหตุ', and 'เวลาเกิดเหตุ' into a new datetime column.\n",
    "    Uses the year from 'ชื่อเทศกาล', the numeric day, and the start time from the time range.\n",
    "    Drops rows with 'ไม่ทราบ' in 'เวลาเกิดเหตุ'.\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import re\n",
    "\n",
    "    print(\"=== Cleaning and Combining to 'datetime' ===\")\n",
    "\n",
    "    # Extract 'year' from festival string\n",
    "    df['year'] = df['ชื่อเทศกาล'].str.extract(r'ปีใหม่\\s*(\\d{2})').astype(float) + 2500 - 543\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    df.drop(columns=['ชื่อเทศกาล'], inplace=True)\n",
    "    print(\"Dropped 'ชื่อเทศกาล' column.\")\n",
    "\n",
    "    # Rename 'วันที่เกิดเหตุ' to 'day_of_month'\n",
    "    df.rename(columns={'วันที่เกิดเหตุ': 'day_of_month'}, inplace=True)\n",
    "    # validate day_of_month is 1-31\n",
    "    # print(df[~df['day_of_month'].isin(range(1, 32))])\n",
    "    df = df[df['day_of_month'].isin(range(1, 32))]\n",
    "\n",
    "    # New column 'month'\n",
    "    # if day_of_month is 1-15, month is 1, else month is 12\n",
    "    df['month'] = df['day_of_month'].apply(lambda x: 1 if x <= 15 else 12)\n",
    "\n",
    "    # Clean time format and extract 'start_time'\n",
    "    def extract_start_time(time_str):\n",
    "        if not isinstance(time_str, str) or 'ไม่ทราบ' in time_str:\n",
    "            return None\n",
    "        time_str = re.sub(r'[^0-9:\\-]', '', time_str)\n",
    "        try:\n",
    "            # special_case: if time_str is start with 24, minus 24\n",
    "            if time_str.startswith('24'):\n",
    "                time_str = time_str.replace('24', '00')\n",
    "            return time_str.split('-')[0][:2]\n",
    "        except:\n",
    "            return None\n",
    "    df['hour'] = df['เวลาเกิดเหตุ'].apply(extract_start_time)\n",
    "    print(\"Extracted 'hour' column.\")\n",
    "    df.drop(columns=['เวลาเกิดเหตุ'], inplace=True)\n",
    "    print(\"Dropped 'เวลาเกิดเหตุ' columns.\")\n",
    "\n",
    "    # Drop 'year' column\n",
    "    df.drop(columns=['year'], inplace=True)\n",
    "    print(\"Dropped 'year' column.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_cleaned = clean_datetime(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 'เพศ' and droped to new binary 'sex' column.\n"
     ]
    }
   ],
   "source": [
    "def clean_gender(df, column='เพศ'):\n",
    "    \"\"\"\n",
    "    Converts Thai gender column to binary 'sex' column.\n",
    "    'ชาย' → 1 (male), 'หญิง' → 0 (female)\n",
    "    \"\"\"\n",
    "    gender_map = {'ชาย': 1, 'หญิง': 0}\n",
    "    df['sex'] = df[column].map(gender_map)\n",
    "    df.drop(columns=[column], inplace=True)\n",
    "    print(\"Converted 'เพศ' and droped to new binary 'sex' column.\")\n",
    "    return df\n",
    "\n",
    "df_cleaned = clean_gender(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 'จังหวัด' and 'ชื่อโรงพยาบาลที่รับผู้บาดเจ็บ' columns.\n"
     ]
    }
   ],
   "source": [
    "def clean_use_id_instead(df):\n",
    "    \"\"\"\n",
    "    Drop 'จังหวัด' and 'ชื่อโรงพยาบาลที่รับผู้บาดเจ็บ' cols.\n",
    "    'จังหวัด' use-> 'รหัสจังหวัด'\n",
    "    'ชื่อโรงพยาบาลที่รับผู้บาดเจ็บ' use-> 'รหัส รพ.'\n",
    "    \"\"\"\n",
    "    # drop 'จังหวัด' 'ชื่อโรงพยาบาลที่รับผู้บาดเจ็บ' cols\n",
    "    df.drop(columns=['จังหวัด', 'ชื่อโรงพยาบาลที่รับผู้บาดเจ็บ'], inplace=True)\n",
    "    print(\"Dropped 'จังหวัด' and 'ชื่อโรงพยาบาลที่รับผู้บาดเจ็บ' columns.\")\n",
    "    return df\n",
    "\n",
    "df_cleaned = clean_use_id_instead(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for column 'รหัสจังหวัด':\n",
      "[10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 30 31 32 33 34 35\n",
      " 36 37 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 60 61\n",
      " 62 63 64 65 66 67 70 71 72 73 74 75 76 77 80 81 82 83 84 85 86 90 91 92\n",
      " 93 94 95 96 97]\n",
      "------------------------------\n",
      "Unique values for column 'รหัส รพ.':\n",
      "[11630 11548 11616 ... 15693 24300 28817]\n",
      "------------------------------\n",
      "Unique values for column 'day_of_month':\n",
      "[ 3  2 30 31 29  1 28  4  5 27]\n",
      "------------------------------\n",
      "Unique values for column 'อายุ':\n",
      "[54 13 81 35 22 27 31 26 45 59 41 42 21 28 17 16 19 20 39 34 58 44 15 33\n",
      " 32 29 25 18 24 50 65 30 62 11 23 36 53  3 51  2 38 64 55 40 46 52 88 71\n",
      " 63 69  6 43 48 47  9  8 10 12 56  7 61 37 14 60 67 57 70  4 87  5 49  1\n",
      " 83 82 78 66 72 80 77 68 84 98 74 73 85 79 99 76 75 92 96 89 93 86 97 90\n",
      " 91 94 95]\n",
      "------------------------------\n",
      "Unique values for column 'ถนนที่เกิดเหตุ':\n",
      "['ในเมือง' 'ไม่ทราบ' 'ทางหลวง' 'ชนบท']\n",
      "------------------------------\n",
      "Unique values for column 'สถานะ':\n",
      "['ผู้ชับขี่' 'คนเดินเท้า' 'ผู้โดยสาร' 'ไม่ทราบ']\n",
      "------------------------------\n",
      "Unique values for column 'รถผู้บาดเจ็บ':\n",
      "['รถเก๋ง/แท็กซี่' 'จักรยานยนต์' 'ไม่มี/ล้มเอง' 'รถจักรยาน'\n",
      " 'รถโดยสาร 4 ล้อ' 'สามล้อเครื่อง' 'ไม่ทราบ' 'อื่นๆ' 'สามล้อถีบ' 'ปิคอัพ'\n",
      " 'รถโดยสารใหญ่' 'รถตู้' 'รถบรรทุก']\n",
      "------------------------------\n",
      "Unique values for column 'รถคู่กรณี':\n",
      "['ไม่มี/ล้มเอง' 'รถตู้' 'ปิคอัพ' 'รถเก๋ง/แท็กซี่' 'รถบรรทุก' 'จักรยานยนต์'\n",
      " 'อื่นๆ' 'ไม่ทราบ' 'สามล้อถีบ' 'รถโดยสารใหญ่' 'รถจักรยาน' 'รถโดยสาร 4 ล้อ'\n",
      " 'สามล้อเครื่อง']\n",
      "------------------------------\n",
      "Unique values for column 'มาตรการ':\n",
      "['เข็มขัด' 'ใส่หมวก' 'ไม่ทราบ' 'ไม่ใส่']\n",
      "------------------------------\n",
      "Unique values for column 'การดื่มสุรา':\n",
      "['ไม่ดื่ม' 'ดื่ม' 'ไม่ทราบ']\n",
      "------------------------------\n",
      "Unique values for column 'การนำส่ง':\n",
      "['ผู้ประสบเหตุ/ญาติ' 'มูลนิธิ/อาสาสมัคร' 'FR' 'เจ้าหน้าที่ตำรวจ' 'ALS'\n",
      " 'ไม่นำส่ง' 'BLS' 'เสียชีวิตที่เกิดเหตุ' 'ILS']\n",
      "------------------------------\n",
      "Unique values for column 'Refer-Admit':\n",
      "['ไม่' 'admit' 'ส่งต่อก่อน admit' 'ส่งต่อหลัง admit']\n",
      "------------------------------\n",
      "Unique values for column 'จำนวนวันรักษา':\n",
      "[ 0  1  5  2 15  3  4  8 21 10 31  9  6  7 13 23 26 25 12 11 30 16 17 14\n",
      " 18 22 19 24 20 27 28 29]\n",
      "------------------------------\n",
      "Unique values for column 'survived':\n",
      "[1 0]\n",
      "------------------------------\n",
      "Unique values for column 'month':\n",
      "[ 1 12]\n",
      "------------------------------\n",
      "Unique values for column 'hour':\n",
      "['12' '21' '11' '02' '04' '22' '10' '17' '16' '14' '13' '15' '00' '03'\n",
      " '23' '20' '08' '09' '19' '01' None '05' '06' '07' '18']\n",
      "------------------------------\n",
      "Unique values for column 'sex':\n",
      "[0 1]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_unique_values_per_column(df):\n",
    "    \"\"\"\n",
    "    Prints the unique values for each column in the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        print(f\"Unique values for column '{column}':\")\n",
    "        print(df[column].unique())\n",
    "        print(\"-\" * 30) # Separator for readability\n",
    "\n",
    "# Example usage (assuming df_cleaned is your DataFrame):\n",
    "print_unique_values_per_column(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Column: 'รหัสจังหวัด' ---\n",
      "รหัสจังหวัด\n",
      "30    9686\n",
      "50    7716\n",
      "20    7051\n",
      "40    6607\n",
      "31    6022\n",
      "      ... \n",
      "91     765\n",
      "23     764\n",
      "37     761\n",
      "75     558\n",
      "97     516\n",
      "Name: count, Length: 77, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'รหัส รพ.' ---\n",
      "รหัส รพ.\n",
      "10819    1726\n",
      "10741    1395\n",
      "10670    1217\n",
      "10666    1206\n",
      "10682    1123\n",
      "         ... \n",
      "14612       1\n",
      "12057       1\n",
      "11961       1\n",
      "12244       1\n",
      "11901       1\n",
      "Name: count, Length: 1204, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'day_of_month' ---\n",
      "day_of_month\n",
      "31    40981\n",
      "1     38582\n",
      "30    34475\n",
      "2     24279\n",
      "29    22394\n",
      "3     15928\n",
      "4     11749\n",
      "28    11260\n",
      "27     6112\n",
      "5      4091\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'อายุ' ---\n",
      "อายุ\n",
      "17    8780\n",
      "18    8705\n",
      "16    8537\n",
      "15    7851\n",
      "19    7641\n",
      "      ... \n",
      "91       3\n",
      "94       3\n",
      "92       2\n",
      "95       2\n",
      "96       1\n",
      "Name: count, Length: 99, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'ถนนที่เกิดเหตุ' ---\n",
      "ถนนที่เกิดเหตุ\n",
      "ชนบท       103577\n",
      "ทางหลวง     59835\n",
      "ในเมือง     41011\n",
      "ไม่ทราบ      5428\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'สถานะ' ---\n",
      "สถานะ\n",
      "ผู้ชับขี่     146568\n",
      "ผู้โดยสาร      54319\n",
      "คนเดินเท้า      7226\n",
      "ไม่ทราบ         1738\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'รถผู้บาดเจ็บ' ---\n",
      "รถผู้บาดเจ็บ\n",
      "จักรยานยนต์       167302\n",
      "ปิคอัพ             12558\n",
      "รถจักรยาน           9564\n",
      "ไม่มี/ล้มเอง        6329\n",
      "รถเก๋ง/แท็กซี่      5974\n",
      "อื่นๆ               2085\n",
      "สามล้อเครื่อง       1860\n",
      "รถตู้               1055\n",
      "ไม่ทราบ              993\n",
      "รถโดยสารใหญ่         803\n",
      "รถบรรทุก             637\n",
      "รถโดยสาร 4 ล้อ       512\n",
      "สามล้อถีบ            179\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'รถคู่กรณี' ---\n",
      "รถคู่กรณี\n",
      "ไม่มี/ล้มเอง      113652\n",
      "จักรยานยนต์        33636\n",
      "ปิคอัพ             25003\n",
      "รถเก๋ง/แท็กซี่     13711\n",
      "อื่นๆ               9428\n",
      "ไม่ทราบ             5831\n",
      "รถบรรทุก            2636\n",
      "รถจักรยาน           2037\n",
      "รถตู้               1211\n",
      "สามล้อเครื่อง       1050\n",
      "รถโดยสารใหญ่         775\n",
      "รถโดยสาร 4 ล้อ       603\n",
      "สามล้อถีบ            278\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'มาตรการ' ---\n",
      "มาตรการ\n",
      "ไม่ใส่     161020\n",
      "ใส่หมวก     31468\n",
      "ไม่ทราบ     12160\n",
      "เข็มขัด      5203\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'การดื่มสุรา' ---\n",
      "การดื่มสุรา\n",
      "ไม่ดื่ม    123533\n",
      "ดื่ม        73831\n",
      "ไม่ทราบ     12487\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'การนำส่ง' ---\n",
      "การนำส่ง\n",
      "ผู้ประสบเหตุ/ญาติ       124028\n",
      "FR                       30604\n",
      "มูลนิธิ/อาสาสมัคร        23417\n",
      "BLS                      11409\n",
      "ALS                      10207\n",
      "ไม่นำส่ง                  5779\n",
      "เจ้าหน้าที่ตำรวจ          3038\n",
      "ILS                        863\n",
      "เสียชีวิตที่เกิดเหตุ       506\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'Refer-Admit' ---\n",
      "Refer-Admit\n",
      "ไม่                 158064\n",
      "admit                34261\n",
      "ส่งต่อก่อน admit     17399\n",
      "ส่งต่อหลัง admit       127\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'จำนวนวันรักษา' ---\n",
      "จำนวนวันรักษา\n",
      "0     187090\n",
      "1      11895\n",
      "2       3422\n",
      "3       2005\n",
      "4       1110\n",
      "5        934\n",
      "7        709\n",
      "6        578\n",
      "8        333\n",
      "10       289\n",
      "9        224\n",
      "31       176\n",
      "12       166\n",
      "11       133\n",
      "14       120\n",
      "15       100\n",
      "13        87\n",
      "16        79\n",
      "18        59\n",
      "17        56\n",
      "20        46\n",
      "21        31\n",
      "22        30\n",
      "19        29\n",
      "30        25\n",
      "26        22\n",
      "25        22\n",
      "24        21\n",
      "23        19\n",
      "27        17\n",
      "28        17\n",
      "29         7\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'survived' ---\n",
      "survived\n",
      "1    206364\n",
      "0      3487\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'month' ---\n",
      "month\n",
      "12    115222\n",
      "1      94629\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'hour' ---\n",
      "hour\n",
      "18      16471\n",
      "17      16132\n",
      "16      14945\n",
      "19      13948\n",
      "15      12074\n",
      "20      11596\n",
      "14      10489\n",
      "13       9869\n",
      "12       9841\n",
      "21       9592\n",
      "11       9040\n",
      "00       8526\n",
      "10       8507\n",
      "22       8129\n",
      "08       7499\n",
      "09       7422\n",
      "23       6458\n",
      "01       6013\n",
      "07       5709\n",
      "02       4625\n",
      "03       3410\n",
      "06       3022\n",
      "04       2643\n",
      "05       2448\n",
      "None     1443\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "--- Column: 'sex' ---\n",
      "sex\n",
      "1    141707\n",
      "0     68144\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_column_value_counts(df, top_n=None):\n",
    "    \"\"\"\n",
    "    Prints unique value counts for each column in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        top_n (int, optional): If specified, limits output to top N most frequent values per column.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        print(f\"--- Column: '{col}' ---\")\n",
    "        value_counts = df[col].value_counts(dropna=False)\n",
    "        if top_n is not None:\n",
    "            value_counts = value_counts.head(top_n)\n",
    "        print(value_counts)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "print_column_value_counts(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Q1...\n",
      "[Q1] RF ROC AUC: 0.718\n",
      "\n",
      "[Q1] Top 20 predictive factors:\n",
      "อายุ                        0.367852\n",
      "hour                        0.261709\n",
      "day_of_month                0.127402\n",
      "drunk                       0.029181\n",
      "sex                         0.017899\n",
      "month                       0.016262\n",
      "helmet                      0.011078\n",
      "ถนนที่เกิดเหตุ_ทางหลวง      0.009283\n",
      "สถานะ_ผู้ชับขี่             0.009164\n",
      "รถผู้บาดเจ็บ_จักรยานยนต์    0.009111\n",
      "รถคู่กรณี_ปิคอัพ            0.008399\n",
      "สถานะ_ผู้โดยสาร             0.008394\n",
      "รถคู่กรณี_รถเก๋ง/แท็กซี่    0.008283\n",
      "ถนนที่เกิดเหตุ_ชนบท         0.007801\n",
      "รถคู่กรณี_รถบรรทุก          0.007466\n",
      "รถคู่กรณี_จักรยานยนต์       0.007173\n",
      "รถผู้บาดเจ็บ_ปิคอัพ         0.007099\n",
      "seatbelt                    0.006728\n",
      "รถคู่กรณี_ไม่ทราบ           0.006555\n",
      "รถคู่กรณี_ไม่มี/ล้มเอง      0.006440\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 0. Load and preprocess data\n",
    "# Assume df_cleaned is already loaded in the environment\n",
    "\n",
    "df = df_cleaned.copy()\n",
    "# Drop post-outcome leakage\n",
    "for col in ['การนำส่ง', 'จำนวนวันรักษา', 'Refer-Admit']:\n",
    "    if col in df:\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# Convert time columns to numeric\n",
    "df['hour'] = pd.to_numeric(df['hour'], errors='coerce')\n",
    "df['day_of_month'] = pd.to_numeric(df['day_of_month'], errors='coerce')\n",
    "df['month'] = pd.to_numeric(df['month'], errors='coerce')\n",
    "\n",
    "# Create binary treatment columns\n",
    "df['helmet'] = (df['มาตรการ'] == 'ใส่หมวก').astype(int)\n",
    "df['seatbelt'] = (df['มาตรการ'] == 'เข็มขัด').astype(int)\n",
    "df['drunk'] = (df['การดื่มสุรา'] == 'ดื่ม').astype(int)\n",
    "\n",
    "# Common pre-treatment covariates\n",
    "covariates = [\n",
    "    'อายุ', 'sex', 'hour', 'month', 'day_of_month',\n",
    "    'ถนนที่เกิดเหตุ', 'สถานะ', 'รถผู้บาดเจ็บ', 'รถคู่กรณี'\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Q1: Feature importance via Random Forest\n",
    "# ----------------------------------------------------------------------------\n",
    "print('Start Q1...')\n",
    "# Prepare X, y\n",
    "y = df['survived']\n",
    "X = df[covariates + ['helmet', 'seatbelt', 'drunk']]\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_features = ['อายุ', 'sex', 'hour', 'month', 'day_of_month', 'helmet', 'seatbelt', 'drunk']\n",
    "categorical_features = [col for col in covariates if df[col].dtype == object]\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=200, random_state=42))\n",
    "])\n",
    "\n",
    "# Split, train, evaluate\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "y_pred = pipeline_rf.predict_proba(X_test)[:,1]\n",
    "print(f\"[Q1] RF ROC AUC: {roc_auc_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "# Extract feature importances\n",
    "rf = pipeline_rf.named_steps['classifier']\n",
    "feature_names = (\n",
    "    numeric_features +\n",
    "    list(pipeline_rf.named_steps['preprocessor']\n",
    "         .transformers_[1][1]\n",
    "         .named_steps['onehot']\n",
    "         .get_feature_names_out(categorical_features))\n",
    ")\n",
    "importances = rf.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "print(\"\\n[Q1] Top 20 predictive factors:\")\n",
    "print(feat_imp.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Q2...\n",
      "\n",
      "[Q2] Helmet OR: 2.65, 95% CI [2.30, 3.05]\n",
      "Start Q3...\n",
      "[Q3] Seatbelt OR: 2.85, 95% CI [2.25, 3.60]\n",
      "Start Q4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pupipatsingkhorn/miniconda3/envs/datascience/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q4] Drunk OR: 1.37, 95% CI [1.27, 1.49]\n",
      "Start Q5...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m yh = df_h[\u001b[33m'\u001b[39m\u001b[33msurvived\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     52\u001b[39m Xh_const = sm.add_constant(Xh)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m model_h = \u001b[43msm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLogit\u001b[49m\u001b[43m(\u001b[49m\u001b[43myh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXh_const\u001b[49m\u001b[43m)\u001b[49m.fit(disp=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Extract hospital coefficients\u001b[39;00m\n\u001b[32m     55\u001b[39m hosp_coefs = model_h.params.filter(like=\u001b[33m'\u001b[39m\u001b[33mรหัส รพ.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:475\u001b[39m, in \u001b[36mBinaryModel.__init__\u001b[39m\u001b[34m(self, endog, exog, offset, check_rank, **kwargs)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, offset=\u001b[38;5;28;01mNone\u001b[39;00m, check_rank=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs):\n\u001b[32m    473\u001b[39m     \u001b[38;5;66;03m# unconditional check, requires no extra kwargs added by subclasses\u001b[39;00m\n\u001b[32m    474\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_kwargs(kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m, MultinomialModel):\n\u001b[32m    478\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all((\u001b[38;5;28mself\u001b[39m.endog >= \u001b[32m0\u001b[39m) & (\u001b[38;5;28mself\u001b[39m.endog <= \u001b[32m1\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:185\u001b[39m, in \u001b[36mDiscreteModel.__init__\u001b[39m\u001b[34m(self, endog, exog, check_rank, **kwargs)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, check_rank=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs):\n\u001b[32m    184\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_rank = check_rank\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mself\u001b[39m.raise_on_perfect_prediction = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# keep for backwards compat\u001b[39;00m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mself\u001b[39m.k_extra = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/statsmodels/base/model.py:270\u001b[39m, in \u001b[36mLikelihoodModel.__init__\u001b[39m\u001b[34m(self, endog, exog, **kwargs)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28mself\u001b[39m.initialize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/statsmodels/base/model.py:95\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, endog, exog, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m missing = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mmissing\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     94\u001b[39m hasconst = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mhasconst\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m                              \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m.k_constant = \u001b[38;5;28mself\u001b[39m.data.k_constant\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m.exog = \u001b[38;5;28mself\u001b[39m.data.exog\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/statsmodels/base/model.py:135\u001b[39m, in \u001b[36mModel._handle_data\u001b[39m\u001b[34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     data = \u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/statsmodels/base/data.py:675\u001b[39m, in \u001b[36mhandle_data\u001b[39m\u001b[34m(endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m    672\u001b[39m     exog = np.asarray(exog)\n\u001b[32m    674\u001b[39m klass = handle_data_class_factory(endog, exog)\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m             \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/statsmodels/base/data.py:84\u001b[39m, in \u001b[36mModelData.__init__\u001b[39m\u001b[34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m.orig_endog = endog\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mself\u001b[39m.orig_exog = exog\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28mself\u001b[39m.endog, \u001b[38;5;28mself\u001b[39m.exog = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_endog_exog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m.const_idx = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.k_constant = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datascience/lib/python3.11/site-packages/statsmodels/base/data.py:509\u001b[39m, in \u001b[36mPandasData._convert_endog_exog\u001b[39m\u001b[34m(self, endog, exog)\u001b[39m\n\u001b[32m    507\u001b[39m exog = exog \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np.asarray(exog)\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endog.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exog.dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPandas data cast to numpy dtype of object. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33mCheck input data with np.asarray(data).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()._convert_endog_exog(endog, exog)\n",
      "\u001b[31mValueError\u001b[39m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Helper: logistic regression effect estimation (odds ratio + 95% CI)\n",
    "# ----------------------------------------------------------------------------\n",
    "def logistic_effect(df_sub, treatment, covariates):\n",
    "    # Drop missing\n",
    "    cols = [treatment, 'survived'] + covariates\n",
    "    df0 = df_sub[cols].dropna()\n",
    "    X = pd.get_dummies(df0[covariates + [treatment]], drop_first=True)\n",
    "    X = X.astype(float)\n",
    "    y = df0['survived']\n",
    "    X_const = sm.add_constant(X)\n",
    "    model = sm.Logit(y.astype(float), X_const.astype(float)).fit(disp=False)\n",
    "    coef = model.params[treatment]\n",
    "    conf = model.conf_int().loc[treatment]\n",
    "    OR = np.exp(coef)\n",
    "    CI = np.exp(conf)\n",
    "    return OR, CI, model\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Q2: Helmet effect in motorcycle accidents\n",
    "# ----------------------------------------------------------------------------\n",
    "print('Start Q2...')\n",
    "mask = df['รถผู้บาดเจ็บ'] == 'จักรยานยนต์'\n",
    "OR_h, CI_h, _ = logistic_effect(df[mask], 'helmet', covariates)\n",
    "print(f\"\\n[Q2] Helmet OR: {OR_h:.2f}, 95% CI [{CI_h[0]:.2f}, {CI_h[1]:.2f}]\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Q3: Seatbelt effect in car accidents\n",
    "# ----------------------------------------------------------------------------\n",
    "print('Start Q3...')\n",
    "car_types = ['รถเก๋ง/แท็กซี่','ปิคอัพ','รถตู้','รถโดยสาร 4 ล้อ','รถโดยสารใหญ่','รถบรรทุก']\n",
    "mask = df['รถผู้บาดเจ็บ'].isin(car_types)\n",
    "OR_s, CI_s, _ = logistic_effect(df[mask], 'seatbelt', covariates)\n",
    "print(f\"[Q3] Seatbelt OR: {OR_s:.2f}, 95% CI [{CI_s[0]:.2f}, {CI_s[1]:.2f}]\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Q4: Alcohol effect overall\n",
    "# ----------------------------------------------------------------------------\n",
    "print('Start Q4...')\n",
    "OR_d, CI_d, _ = logistic_effect(df, 'drunk', covariates)\n",
    "print(f\"[Q4] Drunk OR: {OR_d:.2f}, 95% CI [{CI_d[0]:.2f}, {CI_d[1]:.2f}]\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Q5: Hospital effect on survival\n",
    "# ----------------------------------------------------------------------------\n",
    "print('Start Q5...')\n",
    "# One-hot encode hospital and covariates\n",
    "cols = ['รหัส รพ.'] + covariates + ['survived']\n",
    "df_h = df[cols].dropna()\n",
    "Xh = pd.get_dummies(df_h[['รหัส รพ.'] + covariates], drop_first=True)\n",
    "yh = df_h['survived']\n",
    "Xh_const = sm.add_constant(Xh)\n",
    "model_h = sm.Logit(yh, Xh_const).fit(disp=False)\n",
    "# Extract hospital coefficients\n",
    "hosp_coefs = model_h.params.filter(like='รหัส รพ.')\n",
    "hosp_or = np.exp(hosp_coefs).sort_values()\n",
    "print(\"\\n[Q5] Top 5 hospitals (highest OR):\")\n",
    "print(hosp_or.tail(5))\n",
    "print(\"\\n[Q5] Bottom 5 hospitals (lowest OR):\")\n",
    "print(hosp_or.head(5))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Q6: Additional insights\n",
    "# ----------------------------------------------------------------------------\n",
    "print('Start Q6...')\n",
    "print(\"\\n[Q6] Survival rate by road type:\")\n",
    "print(df.groupby('ถนนที่เกิดเหตุ')['survived'].mean().sort_values())\n",
    "\n",
    "print(\"\\n[Q6] Survival rate by role:\")\n",
    "print(df.groupby('สถานะ')['survived'].mean().sort_values())\n",
    "\n",
    "print(\"\\n[Q6] Survival rate by day_of_month:\")\n",
    "print(df.groupby('day_of_month')['survived'].mean().sort_values().head(5))\n",
    "print(df.groupby('day_of_month')['survived'].mean().sort_values().tail(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

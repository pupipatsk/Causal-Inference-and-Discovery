{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 1186\n",
      "After dedup: 213764\n",
      "Age 0 count: 3912\n",
      "After age filter: 209852\n",
      "Survival distribution:\n",
      " survived\n",
      "1    206365\n",
      "0      3487\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Q1: Feature Importances ===\n",
      "Top 20 factors affecting survivability:\n",
      " day_of_month                   0.267986\n",
      "hour                           0.134792\n",
      "อายุ                           0.086519\n",
      "การดื่มสุรา_ไม่ทราบ            0.068351\n",
      "sex                            0.033993\n",
      "ถนนที่เกิดเหตุ_ทางหลวง         0.030827\n",
      "รถผู้บาดเจ็บ_ปิคอัพ            0.024408\n",
      "ถนนที่เกิดเหตุ_ในเมือง         0.022526\n",
      "รถคู่กรณี_ปิคอัพ               0.021277\n",
      "มาตรการ_ไม่ทราบ                0.020664\n",
      "สถานะ_ผู้ชับขี่                0.020619\n",
      "รถคู่กรณี_รถเก๋ง/แท็กซี่       0.020450\n",
      "มาตรการ_ไม่ใส่                 0.019593\n",
      "สถานะ_ผู้โดยสาร                0.018583\n",
      "การดื่มสุรา_ไม่ดื่ม            0.018096\n",
      "รถผู้บาดเจ็บ_รถเก๋ง/แท็กซี่    0.017981\n",
      "รถผู้บาดเจ็บ_รถจักรยาน         0.016421\n",
      "รถคู่กรณี_ไม่มี/ล้มเอง         0.015624\n",
      "รถคู่กรณี_ไม่ทราบ              0.015082\n",
      "รถคู่กรณี_รถบรรทุก             0.014999\n",
      "dtype: float64\n",
      "\n",
      "=== Q2: Helmet Effect ===\n",
      "Starting causal effect estimation...\n",
      "Treatment variable: helmet\n",
      "Outcome variable: survived\n",
      "Number of confounders: 7\n",
      "Dropping missing values...\n",
      "Remaining samples after dropping NA: 167302\n",
      "Creating causal model...\n",
      "Identifying causal effect...\n",
      "Estimating effect using PSM...\n",
      "helmet ATE: 0.0113\n",
      "Running refutation tests...\n",
      "1. Placebo treatment refutation:\n",
      "Refute: Use a Placebo Treatment\n",
      "Estimated effect:0.011332799368806112\n",
      "New effect:0.00017644738257761405\n",
      "p value:0.8999999999999999\n",
      "\n",
      "2. Random common cause refutation:\n",
      "Refute: Add a random common cause\n",
      "Estimated effect:0.011332799368806112\n",
      "New effect:0.011332799368806115\n",
      "p value:1.0\n",
      "\n",
      "Computing naive difference in means...\n",
      "Naive diff-in-means: 0.0075\n",
      "Causal effect estimation complete.\n",
      "\n",
      "=== Q3: Seatbelt Effect ===\n",
      "Starting causal effect estimation...\n",
      "Treatment variable: seatbelt\n",
      "Outcome variable: survived\n",
      "Number of confounders: 7\n",
      "Dropping missing values...\n",
      "Remaining samples after dropping NA: 19587\n",
      "Creating causal model...\n",
      "Identifying causal effect...\n",
      "Estimating effect using PSM...\n",
      "seatbelt ATE: 0.0746\n",
      "Running refutation tests...\n",
      "1. Placebo treatment refutation:\n",
      "Refute: Use a Placebo Treatment\n",
      "Estimated effect:0.07459028947771482\n",
      "New effect:0.009035074283963855\n",
      "p value:0.72\n",
      "\n",
      "2. Random common cause refutation:\n",
      "Refute: Add a random common cause\n",
      "Estimated effect:0.07459028947771482\n",
      "New effect:0.07459028947771482\n",
      "p value:1.0\n",
      "\n",
      "Computing naive difference in means...\n",
      "Naive diff-in-means: 0.0191\n",
      "Causal effect estimation complete.\n",
      "\n",
      "=== Q4: Alcohol Effect ===\n",
      "Starting causal effect estimation...\n",
      "Treatment variable: alcohol\n",
      "Outcome variable: survived\n",
      "Number of confounders: 17\n",
      "Dropping missing values...\n",
      "Remaining samples after dropping NA: 209851\n",
      "Creating causal model...\n",
      "Identifying causal effect...\n",
      "Estimating effect using PSM...\n",
      "alcohol ATE: 0.0060\n",
      "Running refutation tests...\n",
      "1. Placebo treatment refutation:\n",
      "Refute: Use a Placebo Treatment\n",
      "Estimated effect:0.006018556023083045\n",
      "New effect:-0.0002500345483223811\n",
      "p value:0.86\n",
      "\n",
      "2. Random common cause refutation:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np  # numpy documentation: https://numpy.org/doc/\n",
    "import pandas as pd  # pandas documentation: https://pandas.pydata.org/docs/\n",
    "import matplotlib.pyplot as plt  # matplotlib documentation: https://matplotlib.org/stable/api/pyplot_api.html\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  # scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.linear_model import LogisticRegression  # scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dowhy import CausalModel  # dowhy: https://microsoft.github.io/dowhy/\n",
    "\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load raw Excel data.\"\"\"\n",
    "    return pd.read_excel(path)\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform data cleaning:\n",
    "    1. Remove duplicates.\n",
    "    2. Filter valid ages.\n",
    "    3. Create binary survival label.\n",
    "    4. Extract month and hour correctly.\n",
    "    5. Encode sex.\n",
    "    6. Drop unneeded columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # 1. Duplicates\n",
    "    print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"After dedup: {df.shape[0]}\")\n",
    "    \n",
    "    # 2. Age validity\n",
    "    # Convert to numeric and filter invalid ages\n",
    "    df['อายุ'] = pd.to_numeric(df['อายุ'], errors='coerce')\n",
    "    print(f\"Age 0 count: {(df['อายุ'] == 0).sum()}\")\n",
    "    df = df[df['อายุ'] > 0].dropna(subset=['อายุ'])\n",
    "    print(f\"After age filter: {df.shape[0]}\")\n",
    "    # Define bins and their midpoints\n",
    "    age_bins = [0, 15, 25, 65, 200]\n",
    "    bin_midpoints = {\n",
    "        pd.Interval(0, 15, closed='left'): 7.5, \n",
    "        pd.Interval(15, 25, closed='left'): 20,\n",
    "        pd.Interval(25, 65, closed='left'): 45,\n",
    "        pd.Interval(65, 200, closed='left'): 80\n",
    "    }\n",
    "    # Bin ages and map to midpoints\n",
    "    binned = pd.cut(df['อายุ'], bins=age_bins, right=False)\n",
    "    df['อายุ'] = binned.map(bin_midpoints)\n",
    "    \n",
    "    # 3. Survival label\n",
    "    df['survived'] = (df['ผลการรักษา'] == 'ทุเลา/หาย').astype(int)\n",
    "    print(\"Survival distribution:\\n\", df['survived'].value_counts())\n",
    "    df = df.drop(columns=['ผลการรักษา'])\n",
    "    \n",
    "    # 4. Date/Time processing\n",
    "    df = df.rename(columns={'วันที่เกิดเหตุ': 'day_of_month'})\n",
    "    df = df[df['day_of_month'].between(1, 31)]\n",
    "    # # Map month for New Year period: days 29-31 -> Dec(12), days 1-4 -> Jan(1)\n",
    "    # df['month'] = df['day_of_month'].apply(lambda x: 12 if x >= 29 else (1 if x <= 4 else np.nan))\n",
    "    # df = df.dropna(subset=['month'])\n",
    "    # df['month'] = df['month'].astype(int)\n",
    "    \n",
    "    def extract_hour(s):\n",
    "        if pd.isna(s) or 'ไม่ทราบ' in str(s):\n",
    "            return np.nan\n",
    "        cleaned = re.sub(r'[^0-9:]', '', str(s))\n",
    "        if cleaned.startswith('24:'):\n",
    "            cleaned = '00:' + cleaned[3:]\n",
    "        try:\n",
    "            hour = int(cleaned.split(':')[0])\n",
    "            # Map hours to quarters (0-6, 6-12, 12-18, 18-24)\n",
    "            if 0 <= hour < 6:\n",
    "                return 1  # First quarter (midnight to 6am)\n",
    "            elif 6 <= hour < 12:\n",
    "                return 2  # Second quarter (6am to noon)\n",
    "            elif 12 <= hour < 18:\n",
    "                return 3  # Third quarter (noon to 6pm)\n",
    "            else:\n",
    "                return 4  # Fourth quarter (6pm to midnight)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    df['hour'] = df['เวลาเกิดเหตุ'].apply(extract_hour)\n",
    "    df = df.drop(columns=['เวลาเกิดเหตุ'])\n",
    "    # 5. Gender encoding\n",
    "    df['sex'] = df['เพศ'].map({'ชาย': 1, 'หญิง': 0})\n",
    "    df = df.drop(columns=['เพศ'])\n",
    "    \n",
    "    # 6. Drop irrelevant columns\n",
    "    df = df.drop(columns=['จังหวัด', 'ชื่อโรงพยาบาลที่รับผู้บาดเจ็บ', 'ชื่อเทศกาล'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_importance_rf(df: pd.DataFrame):\n",
    "    \"\"\"Compute and display top-5 feature importances via Random Forest.\"\"\"\n",
    "    feats = ['อายุ', 'sex', 'day_of_month', \n",
    "            #  'month', \n",
    "             'hour',\n",
    "             'ถนนที่เกิดเหตุ', 'สถานะ', 'รถผู้บาดเจ็บ',\n",
    "             'รถคู่กรณี', 'มาตรการ', 'การดื่มสุรา']\n",
    "    sub = df[feats + ['survived']].dropna(subset=['hour'])\n",
    "    sub = pd.get_dummies(sub, columns=[\n",
    "        'ถนนที่เกิดเหตุ', 'สถานะ', 'รถผู้บาดเจ็บ',\n",
    "        'รถคู่กรณี', 'มาตรการ', 'การดื่มสุรา'\n",
    "    ], drop_first=True)\n",
    "    X = sub.drop(columns=['survived'])\n",
    "    y = sub['survived']\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    imp = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    print(\"Top 20 factors affecting survivability:\\n\", imp.sort_values(ascending=False).head(20))\n",
    "\n",
    "\n",
    "def estimate_causal_effect(df: pd.DataFrame, treatment: str, outcome: str, confounders: list):\n",
    "    \"\"\"\n",
    "    Estimate ATE with DoWhy (PSM) and run two refutation tests.\n",
    "    \"\"\"\n",
    "    print(\"Starting causal effect estimation...\")\n",
    "    print(f\"Treatment variable: {treatment}\")\n",
    "    print(f\"Outcome variable: {outcome}\")\n",
    "    print(f\"Number of confounders: {len(confounders)}\")\n",
    "    \n",
    "    print(\"Dropping missing values...\")\n",
    "    data = df.dropna(subset=[treatment, outcome] + confounders)\n",
    "    print(f\"Remaining samples after dropping NA: {len(data)}\")\n",
    "    \n",
    "    print(\"Creating causal model...\")\n",
    "    model = CausalModel(\n",
    "        data=data,\n",
    "        treatment=treatment,\n",
    "        outcome=outcome,\n",
    "        common_causes=confounders\n",
    "    )\n",
    "    \n",
    "    print(\"Identifying causal effect...\")\n",
    "    estimand = model.identify_effect()\n",
    "    \n",
    "    print(\"Estimating effect using PSM...\")\n",
    "    est = model.estimate_effect(estimand, method_name=\"backdoor.propensity_score_matching\")\n",
    "    print(f\"{treatment} ATE: {est.value:.4f}\")\n",
    "    \n",
    "    print(\"Running refutation tests...\")\n",
    "    print(\"1. Placebo treatment refutation:\")\n",
    "    print(model.refute_estimate(estimand, est, method_name=\"placebo_treatment_refuter\"))\n",
    "    print(\"2. Random common cause refutation:\")\n",
    "    print(model.refute_estimate(estimand, est, method_name=\"random_common_cause\"))\n",
    "    \n",
    "    print(\"Computing naive difference in means...\")\n",
    "    naive = data[data[treatment] == 1][outcome].mean() - data[data[treatment] == 0][outcome].mean()\n",
    "    print(f\"Naive diff-in-means: {naive:.4f}\")\n",
    "    print(\"Causal effect estimation complete.\")\n",
    "\n",
    "\n",
    "def hospital_effect(df: pd.DataFrame):\n",
    "    \"\"\"Assess hospital-level deviations from expected survival.\"\"\"\n",
    "    feats = ['อายุ', 'sex', 'ถนนที่เกิดเหตุ', 'สถานะ',\n",
    "             'รถผู้บาดเจ็บ', 'รถคู่กรณี', 'มาตรการ', 'การดื่มสุรา']\n",
    "    sub = df[feats + ['survived', 'รหัส รพ.']].dropna()\n",
    "    sub_enc = pd.get_dummies(sub, columns=feats, drop_first=True)\n",
    "    X = sub_enc.drop(columns=['survived', 'รหัส รพ.'])\n",
    "    y = sub_enc['survived']\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    log = LogisticRegression(max_iter=5000, solver='liblinear').fit(X_scaled, y)\n",
    "    sub['expected'] = log.predict_proba(X_scaled)[:, 1]\n",
    "    rates = sub.groupby('รหัส รพ.').agg(\n",
    "        observed=('survived', 'mean'),\n",
    "        expected=('expected', 'mean')\n",
    "    )\n",
    "    rates['difference'] = rates['observed'] - rates['expected']\n",
    "    print(\"Hospital survival difference std:\", rates['difference'].std())\n",
    "\n",
    "\n",
    "def plot_hourly_survival(df: pd.DataFrame):\n",
    "    \"\"\"Plot survival rate by hour of day.\"\"\"\n",
    "    hr = (\n",
    "        df[df['hour'].notnull()]\n",
    "        .assign(hour=lambda x: x['hour'].astype(int))\n",
    "        .groupby('hour')['survived']\n",
    "        .mean()\n",
    "    )\n",
    "    hr.plot(title='Survival Rate by Hour', xlabel='Hour', ylabel='Survival Rate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def run_q1(df):\n",
    "    \"\"\"Run analysis for Q1: Feature Importances\"\"\"\n",
    "    print(\"\\n=== Q1: Feature Importances ===\")\n",
    "    feature_importance_rf(df)\n",
    "\n",
    "def run_q2(df):\n",
    "    \"\"\"Run analysis for Q2: Helmet Effect\"\"\"\n",
    "    print(\"\\n=== Q2: Helmet Effect ===\")\n",
    "    df_mc = df[df['รถผู้บาดเจ็บ'] == 'จักรยานยนต์'].copy()\n",
    "    df_mc = df_mc[df_mc['มาตรการ'].notna()]\n",
    "    df_mc['helmet'] = (df_mc['มาตรการ'] == 'ใส่หมวก').astype(int)\n",
    "    df_mc_enc = pd.get_dummies(df_mc, columns=['ถนนที่เกิดเหตุ','การดื่มสุรา'], drop_first=True)\n",
    "    conf = ['อายุ','sex'] + [c for c in df_mc_enc.columns if c.startswith(('ถนนที่เกิดเหตุ_','การดื่มสุรา_'))]\n",
    "    estimate_causal_effect(df_mc_enc, 'helmet', 'survived', conf)\n",
    "\n",
    "def run_q3(df):\n",
    "    \"\"\"Run analysis for Q3: Seatbelt Effect\"\"\"\n",
    "    print(\"\\n=== Q3: Seatbelt Effect ===\")\n",
    "    car_types = ['รถเก๋ง/แท็กซี่', 'ปิคอัพ', 'รถตู้']\n",
    "    df_car = df[df['รถผู้บาดเจ็บ'].isin(car_types)].copy()\n",
    "    df_car = df_car[df_car['มาตรการ'].notna()]\n",
    "    df_car['seatbelt'] = (df_car['มาตรการ'] == 'เข็มขัด').astype(int)\n",
    "    df_car_enc = pd.get_dummies(df_car, columns=['ถนนที่เกิดเหตุ','การดื่มสุรา'], drop_first=True)\n",
    "    conf_car = ['อายุ','sex'] + [c for c in df_car_enc.columns if c.startswith(('ถนนที่เกิดเหตุ_','การดื่มสุรา_'))]\n",
    "    estimate_causal_effect(df_car_enc, 'seatbelt', 'survived', conf_car)\n",
    "\n",
    "def run_q4(df):\n",
    "    \"\"\"Run analysis for Q4: Alcohol Effect\"\"\"\n",
    "    print(\"\\n=== Q4: Alcohol Effect ===\")\n",
    "    df_al = df[df['การดื่มสุรา'].notna()].copy()\n",
    "    df_al['alcohol'] = (df_al['การดื่มสุรา'] == 'ดื่ม').astype(int)\n",
    "    df_al_enc = pd.get_dummies(df_al, columns=['ถนนที่เกิดเหตุ','รถผู้บาดเจ็บ'], drop_first=True)\n",
    "    conf_al = ['อายุ','sex'] + [c for c in df_al_enc.columns if c.startswith(('ถนนที่เกิดเหตุ_','รถผู้บาดเจ็บ_'))]\n",
    "    estimate_causal_effect(df_al_enc, 'alcohol', 'survived', conf_al)\n",
    "\n",
    "def run_q5(df):\n",
    "    \"\"\"Run analysis for Q5: Hospital Effect\"\"\"\n",
    "    print(\"\\n=== Q5: Hospital Effect ===\")\n",
    "    hospital_effect(df)\n",
    "\n",
    "def run_q6(df):\n",
    "    \"\"\"Run analysis for Q6: Hourly Survival\"\"\"\n",
    "    print(\"\\n=== Q6: Hourly Survival ===\")\n",
    "    plot_hourly_survival(df)\n",
    "\n",
    "def main():\n",
    "    repo = os.path.dirname(os.getcwd())\n",
    "    path = os.path.join(repo, 'final-project', 'data', 'raw.xlsx')\n",
    "    df = load_data(path)\n",
    "    df = clean_data(df)\n",
    "\n",
    "    run_q1(df)\n",
    "    run_q2(df)\n",
    "    run_q3(df)\n",
    "    run_q4(df)\n",
    "    run_q5(df)\n",
    "    run_q6(df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
